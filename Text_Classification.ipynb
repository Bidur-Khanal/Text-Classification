{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "For reference I followed following resources:\n",
    "\n",
    "https://www.kaggle.com/sudhirnl7/logistic-regression-with-stratifiedkfold\n",
    "https://medium.com/datadriveninvestor/k-fold-and-other-cross-validation-techniques-6c03a2563f1e\n",
    "https://realpython.com/python-keras-text-classification/#defining-a-baseline-model\n",
    "https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "\n",
    "Here, I have tested Used 4 Basic feature extractors:\n",
    "\n",
    "1) Count Vector\n",
    "2) Word IF-IDF\n",
    "3) N-gram IF-IDF\n",
    "4) Character IF-IDF\n",
    "\n",
    "For training, I tested two classifiers:\n",
    "\n",
    "1) Logistic Regression\n",
    "2) Support Vector Machine with Linear Kernel\n",
    "\n",
    "I have attached a pdf (with performance metrices for above features and classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import important modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import  train_test_split, StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data sets\n",
    "train_data= pd.read_csv (r'train_set.csv',encoding='ISO-8859-1')\n",
    "test_data =pd.read_csv(r'test_set.csv',encoding='ISO-8859-1')\n",
    "\n",
    "## we use only train data for training, split it into labels and texts\n",
    "labels= train_data['label']\n",
    "texts= train_data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "# proper split makes our performance estimators (prediction metrices) stable \n",
    "# Here, I have used stratified cross-validation. It forces each fold to have at least m instances of each class, enabling balanced split\n",
    "# Here, train to test split ratio = 4:1\n",
    "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "xtr =[]   #holds train_X, Here, n_splits =5, so, there are 5 different train_X sets\n",
    "ytr =[]   #holds train_Y (labels), Here, n_splits =5, so, there are 5 different train_Y sets\n",
    "xvl =[]   #holds Validation_X, Here, n_splits =5, so, there are 5 different Validation_X sets\n",
    "yvl =[]   #holds Validation_Y (labels), Here, n_splits =5, so, there are 5 different Validation_Y sets\n",
    "i=0       # counter\n",
    "\n",
    "for train_index,test_index in kf.split(texts,labels):\n",
    "   \n",
    "    xtr.append (texts.loc[train_index])\n",
    "    ytr.append (labels.loc[train_index])\n",
    "    xvl.append (texts.loc[test_index])\n",
    "    yvl.append (labels.loc[test_index])\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractors\n",
    "\n",
    "# count vector features\n",
    "def count_vector (xtr,xvl):\n",
    "    vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
    "    vectorizer.fit(xtr)\n",
    "    cnt_vector_xtr=vectorizer.transform(xtr)\n",
    "    cnt_vector_xvl=vectorizer.transform(xvl)\n",
    "    \n",
    "    return vectorizer,cnt_vector_xtr,cnt_vector_xvl\n",
    "\n",
    "# word level tf-idf\n",
    "def word_tfidf( xtr,xvl):\n",
    "\n",
    "    tfidf_vect = TfidfVectorizer()\n",
    "    tfidf_vect.fit(xtr)\n",
    "    xtrain_tfidf =  tfidf_vect.transform(xtr)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(xvl)\n",
    "    \n",
    "    return tfidf_vect, xtrain_tfidf,xvalid_tfidf\n",
    "\n",
    "# ngram level tf-idf \n",
    "def ngram_tfidf(xtr,xvl):\n",
    "    \n",
    "    tfidf_vect_ngram = TfidfVectorizer(ngram_range=(2,3))\n",
    "    tfidf_vect_ngram.fit(xtr)\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(xtr)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(xvl)\n",
    "    \n",
    "    return tfidf_vect_ngram, xtrain_tfidf_ngram,xvalid_tfidf_ngram\n",
    "\n",
    "# characters level tf-idf\n",
    "def character_tfidf(xtr,xvl):\n",
    "    \n",
    "    tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char',ngram_range=(2,3))\n",
    "    tfidf_vect_ngram_chars.fit(xtr)\n",
    "    xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(xtr) \n",
    "    xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(xvl) \n",
    "    \n",
    "    return tfidf_vect_ngram_chars, xtrain_tfidf_ngram_chars, xvalid_tfidf_ngram_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "# Model Trainer and performance estimator\n",
    "def model_trainer(classifier, feature_vector_train, feature_vector_validation,train_y, validation_y):\n",
    "    \n",
    "    \n",
    "    model = classifier.fit (feature_vector_train, train_y)\n",
    "        \n",
    "    acc_score = accuracy_score(validation_y, model.predict(feature_vector_validation))\n",
    "    f1_scr= f1_score(validation_y, model.predict(feature_vector_validation), average='weighted') \n",
    "    \n",
    "    return model, acc_score,f1_scr\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the  0 Fold, Accuracy:  0.9382402707275804\n",
      "For the  0 Fold, F1_score:  0.93842726084675\n",
      "For the  1 Fold, Accuracy:  0.9398814563928873\n",
      "For the  1 Fold, F1_score:  0.9397518251834864\n",
      "For the  2 Fold, Accuracy:  0.9358594411515665\n",
      "For the  2 Fold, F1_score:  0.9358490029502204\n",
      "For the  3 Fold, Accuracy:  0.9377250582503707\n",
      "For the  3 Fold, F1_score:  0.9377846368033108\n",
      "For the  4 Fold, Accuracy:  0.9446799491309877\n",
      "For the  4 Fold, F1_score:  0.944733062185404\n",
      "Mean Accuracy :  0.9392772351306785\n",
      "Mean F1 score :  0.9393091575938344\n"
     ]
    }
   ],
   "source": [
    "# Perform training, check different models and feature vectors and evaluate performance metrices\n",
    "\n",
    "classifier= SVC(kernel='linear') #choose the desired classifier, use sklearn models only, example svm.SVC()\n",
    "cv_accuracy_score =[]\n",
    "cv_f1_score=[]\n",
    "\n",
    "# training and performance evaluation is performed with N-splits (from stratified cross validation)   \n",
    "for index in range (i):\n",
    "        \n",
    "        vector,train_X_feature, test_X_feature= character_tfidf(xtr[index],xvl[index]) # to use other features, only change the function\n",
    "        #example: word_tfidf(xtr[index],xvl[index]) to choose \"word level tf-idf\" feature extractor, retain vector as dictionary\n",
    "                                                                            \n",
    "        \n",
    "        # trainer = model_trainer (classifier, train_X, test_X, train_Y, test_Y)\n",
    "        model, accuracy, F1_Score = model_trainer(classifier,  train_X_feature, test_X_feature, ytr[index], yvl[index])\n",
    "        cv_accuracy_score.append(accuracy)\n",
    "        cv_f1_score.append(F1_Score)\n",
    "        print (\"For the \", index, \"Fold, Accuracy: \", accuracy)\n",
    "        print (\"For the \", index, \"Fold, F1_score: \", F1_Score)\n",
    "\n",
    "Mean_accuracy= np.mean(cv_accuracy_score)\n",
    "Mean_f1_score= np.mean(cv_f1_score)\n",
    "        \n",
    "print (\"Mean Accuracy : \", Mean_accuracy)\n",
    "print (\"Mean F1 score : \", Mean_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the code for test_set.csv\n",
    "\n",
    "test_texts= test_data['text']\n",
    "feature_extractor = vector.transform(test_texts) #vector is retained during training\n",
    "output_class= model.predict(feature_extractor)\n",
    "print (output_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
